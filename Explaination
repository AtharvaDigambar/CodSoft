Loss 
In deep learning, model loss is a measure of how well or poorly your model is performing. It tells you how far off your model's predictions are from the 
actual (true) values.
Think of it this way:
The loss is a number that shows how wrong the model is.
The lower the loss, the better the model is doing.
The higher the loss, the worse the model is doin

Conv2D
Conv2D (2D Convolution) is a convolutional layer that applies filters (kernels) to a 2D input (like an image) to extract important features such as edges, 
textures, and shapes.
It's typically used as the first layer in a CNN (Convolutional Neural Network).
Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))
When you set 32 filters, the Conv2D layer will learn 32 different features from the input image. Each of these filters will produce a separate feature map, 
and the resulting output will have 32 channels.

MaxPooling2D
The MaxPooling2D layer performs a pooling operation by dividing the input into small non-overlapping rectangles (called windows or patches), and for each patch, 
it takes the maximum value. This reduces the spatial size of the input.

Flatten
The Flatten() layer takes the multi-dimensional output from previous layers (like a 2D feature map from a Conv2D layer or a 3D tensor after pooling) 
and converts it into a 1D array (vector).

Dense(10, activation='softmax')
Dense: A dense (fully connected) layer means that each neuron in this layer is connected to every neuron in the previous layer. 
This is the most commonly used layer in neural networks for making predictions after feature extraction (usually done by convolutional layers in CNNs)



activation='relu':
Helps to introduce the Non-linear activation function applied after convolution (helps model learn complex patterns).
Range of ReLU: 
[0,+∞)

Softmax
The softmax activation function is used for multi-class classification problems. It converts the raw outputs (logits) from the dense layer into probabilities 
that sum to 1, where each probability represents the likelihood that the input belongs to each class.

Adam
The Adam optimizer is an adaptive learning rate optimization algorithm that adjusts the learning rate for each parameter individually based on its past gradients.

Embedding Layer?
The Embedding layer maps each word (represented as an integer index) into a dense vector of fixed size. 
It creates a lookup table where each word index corresponds to a learnable vector of numbers.

GlobalAveragePooling1D(),
Global average pooling layer to reduce dimensions

Training Accuracy
This is the accuracy of your model on the training data.
It tells you how well your model is fitting the data it was trained on.
High training accuracy means the model has learned to predict the labels in the training dataset correctly.

Validation Accuracy
This is the accuracy on a separate set of data (called the validation set) that the model has not seen during training.
It tells you how well the model generalizes to new, unseen data.
It's a better indicator of real-world performance.

Training Loss
This is the error (or cost) the model makes on the training data.
It measures how well the model fits the training data.
The goal of training is to minimize this loss by adjusting the model's weights using an optimizer (like Adam or SGD).

Validation Loss
This is the error the model makes on the validation data, which it hasn't seen during training.
It shows how well the model is likely to perform on new, unseen data.
A low validation loss means the model is generalizing well.

MinMaxScaler()
MinMaxScaler is a function from sklearn.preprocessing that scales (normalizes) your data to a fixed range, usually 0 to 1.

append mean in Python
append() is a method used to add an item to the end of a list.

 LSTM?
Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) architecture designed to effectively learn long-term dependencies in sequential data.
The LSTM cell maintains a cell state (a kind of long-term memory) that runs through the entire sequence. This cell state is modified using gates — mathematical structures that decide what to remember, what to forget, and what to output.
There are three primary gates in an LSTM units:
1.Forget Gate
Purpose: Decides what information should be removed from the cell state.
2. Input Gate
Purpose: Decides what new information will be stored in the cell state.
3.  Output Gate
Purpose: Decides what part of the memory to output (i.e., the next hidden state).

What is GRU?
GRU stands for Gated Recurrent Unit, introduced by Cho et al. in 2014.
It’s a type of Recurrent Neural Network (RNN) designed to solve the vanishing gradient problem and handle long-term dependencies, just like LSTM — but with a simpler architecture and fewer parameters.

GRU Working — Step-by-Step (Conceptual)
1. Update Gate (zₜ)
Decides how much of the past information to keep.
If z is close to 1 → keep most of the old info.
If z is close to 0 → replace it with the new input.

2. Reset Gate (rₜ)
Controls how much of the past information to forget.
If r is close to 0 → forget most of the past.
If r is close to 1 → keep it all.

3. New Memory (h̃ₜ)
Combines current input and (optionally) previous hidden state to generate a candidate hidden state (temporary memory).

4. Final Hidden State (hₜ)
Blends the old hidden state and new memory using the update gate.
